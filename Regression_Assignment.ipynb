{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Simple Linear Regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "Simple Linear Regression models the relationship between two variables by fitting a straight line. One variable is independent (X) and the other is dependent (Y). The model predicts Y from X.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "•\tLinearity between X and Y.\n",
        "•\tIndependence of errors.\n",
        "•\tHomoscedasticity (constant variance of errors).\n",
        "•\tErrors are normally distributed.\n",
        "•\tNo significant outliers.\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "The coefficient m in the equation \\( Y = mX + c \\) represents the slope of the line.  \n",
        "\n",
        "It tells us how much Y changes for a one-unit increase in X.  \n",
        "- If m is positive, Y increases as X increases.\n",
        "- If m is negative, Y decreases as X increases.\n",
        "\n",
        "In short:  \n",
        "m = Change in Y / Change in X\n",
        "\n",
        "\n",
        "4. What does the intercept c represent in the equation Y = mX + c?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "The intercept c in the equation \\( Y = mX + c \\) represents the value of Y when X = 0.\n",
        "\n",
        "It is the point where the line crosses the Y-axis.\n",
        "\n",
        "In simple words:  \n",
        "> c is the starting value of Y before any effect of X happens.\n",
        "\n",
        "\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "m=∑(X−Xˉ)(Y−Yˉ)∑(X−Xˉ)2m = \\frac{\\sum (X - \\bar{X})(Y - \\bar{Y})}{\\sum (X - \\bar{X})^2}\n",
        "where Xˉ\\bar{X} and Yˉ\\bar{Y} are the means of X and Y.\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "It minimizes the sum of squared differences between the observed and predicted Y values (errors).\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "R² shows the proportion of variance in Y explained by X.\n",
        "E.g., R² = 0.8 means 80% of the variability in Y is explained by X.\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "\n",
        "It's a regression model where Y depends on two or more independent variables.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "•\tSimple: One independent variable.\n",
        "•\tMultiple: Two or more independent variables.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "•\tLinearity between dependent and independent variables.\n",
        "•\tNo multicollinearity among predictors.\n",
        "•\tIndependence of residuals.\n",
        "•\tHomoscedasticity.\n",
        "•\tNormally distributed residuals.\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "Heteroscedasticity means that the variance of the residuals (errors) is not constant across all levels of the independent variables.  \n",
        "In other words, as X changes, the spread (or scatter) of the errors changes too — it might get wider or narrower.\n",
        "\n",
        "\n",
        "How it affects Multiple Linear Regression:\n",
        "- It violates one of the key assumptions of regression (constant variance).\n",
        "- Leads to **inefficient estimates.\n",
        "- Standard errors of coefficients become wrong, making t-tests and confidence intervals unreliable.\n",
        "- You might think a variable is significant when it’s not, or vice versa.\n",
        "\n",
        "Quick visualization:  \n",
        "In a residual plot:\n",
        "- If the spread of points looks like a funnel (wider or narrower), that's a sign of heteroscedasticity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "•\tRemove or combine correlated variables.\n",
        "•\tUse techniques like Ridge or Lasso Regression.\n",
        "•\tApply Principal Component Analysis (PCA).\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "•\tOne-hot encoding.\n",
        "•\tLabel encoding.\n",
        "•\tOrdinal encoding (for ordered categories).\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "In Multiple Linear Regression, interaction terms are used to model situations where the effect of one independent variable on the dependent variable depends on the value of another independent variable.\n",
        "\n",
        "In simple words:\n",
        "> An interaction term captures how two variables work together to affect Y, beyond their individual effects.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose you are studying how **hours studied (X₁) and sleep hours (X₂) affect exam score (Y).  \n",
        "It might be that studying more improves your score, but **only if you also slept well — that's an interaction!\n",
        "\n",
        "You'd model it like:  \n",
        "\\[\n",
        "Y = b_0 + b_1X_1 + b_2X_2 + b_3(X_1 \\times X_2)\n",
        "\\]\n",
        "Here, \\( b_3 \\) is the **interaction effect**.\n",
        "\n",
        "\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "•\tIn Simple Regression, the intercept is Y when X = 0.\n",
        "•\tIn Multiple Regression, it's Y when all X variables = 0 — sometimes less meaningful.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "\n",
        "In regression analysis, the slope shows the rate of change of the dependent variable (Y) for each one-unit increase in the independent variable (X).\n",
        "\n",
        "In simple words:\n",
        "> The slope tells you how much Y increases (or decreases) when X increases by 1 unit.\n",
        "\n",
        "Significance:\n",
        "- A positive slope means Y increases as X increases.\n",
        "- A negative slope means Y decreases as X increases.\n",
        "- The magnitude of the slope shows how strong the effect of X on Y is.\n",
        "\n",
        "\n",
        "How it affects predictions:\n",
        "- It determines the direction and steepness of the regression line.\n",
        "- Larger absolute values of the slope make predictions change more sharply with changes in X.\n",
        "- If slope = 0, X has no effect on Y.\n",
        "\n",
        "\n",
        "Example:  \n",
        "If your model is:  \n",
        "\\[\n",
        "\\text{Sales} = 200 + 50 \\times \\text{AdvertisingSpend}\n",
        "\\]\n",
        "- Slope = 50.\n",
        "- Means for every extra unit of advertising spending, sales increase by 50 units.\n",
        "\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "In a regression model, the intercept provides the baseline value of the dependent variable (Y) when all independent variables (X) are equal to zero.\n",
        "\n",
        "In simple words:\n",
        "> The intercept tells you \"what is the expected value of Y when X is zero\".\n",
        "\n",
        "Why is it important?\n",
        "- It gives context to the model: where the line (or surface) starts.\n",
        "- It helps anchor the relationship between X and Y.\n",
        "- It can help you interpret predictions — even if X = 0 isn't realistic, it shows where the model's \"origin\" is.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "•\tIt always increases with more variables (even useless ones).\n",
        "•\tIt doesn't indicate if the model is biased.\n",
        "•\tIt can't tell if the model is a good predictive model.\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "A large standard error for a regression coefficient means that the estimate of the coefficient is not very precise.\n",
        "\n",
        "In simple words:\n",
        "> A large standard error suggests there is a lot of uncertainty about the true value of the coefficient.\n",
        "\n",
        "Interpretation:\n",
        "- The coefficient might vary a lot if you repeated the study with different data samples.\n",
        "- The coefficient might not be statistically significant (it could be close to zero).\n",
        "- Confidence intervals around the coefficient would be wider, showing more uncertainty.\n",
        "\n",
        "Why it matters:\n",
        "\n",
        "- A large standard error can make it hard to trust the effect of that variable.\n",
        "- It may indicate problems like multicollinearity (variables being highly correlated) or too much noise in the data.\n",
        "\n",
        "\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "•\tPlot residuals vs. fitted values: a \"fan shape\" suggests heteroscedasticity.\n",
        "•\tImportant because it leads to unreliable hypothesis tests and confidence intervals.\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "If a Multiple Linear Regression model has a high R² but a low adjusted R², it means:\n",
        "\n",
        "R² is high because adding more variables always increases or at least doesn’t decrease R² — even if those variables are not truly helpful.\n",
        "\n",
        "But Adjusted R² penalizes unnecessary variables.  \n",
        "If those added variables don't actually improve the mode, adjusted R² drops.\n",
        "\n",
        "In simple words:\n",
        "> The model might look good at first (high R²), but actually, it's overfitting or including irrelevant predictors (shown by the low adjusted R²).\n",
        "\n",
        "\n",
        "Why is this important?\n",
        "- It tells you that some predictors are not useful.\n",
        "- It suggests that the model is less reliable despite the high R².\n",
        "\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "•\tTo ensure all variables contribute equally to the model.\n",
        "•\tTo speed up convergence during optimization (especially in regularized models).\n",
        "\n",
        "23. What is polynomial regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "Polynomial regression is a type of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial.\n",
        "\n",
        "In simple terms, it fits a curved line (rather than a straight line like linear regression) to the data by including powers of the input variable(s), such as \\( x^2, x^3, \\) etc.\n",
        "\n",
        "General form (for one variable):\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\dots + \\beta_d x^d + \\epsilon\n",
        "\\]\n",
        "\n",
        "Key points:\n",
        "- It is useful when data shows a nonlinear trend.\n",
        "- Despite fitting a curve, it is still linear in the model coefficients**.\n",
        "- It can model more complex relationships but can also overfit if the degree is too high.\n",
        "\n",
        "\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "•\tLinear: Relationship is a straight line.\n",
        "•\tPolynomial: Relationship is a curve (non-linear).\n",
        "\n",
        "\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "Polynomial regression is used when the relationship between the independent variable(s) and the dependent variable is nonlinear, but can still be reasonably approximated by a polynomial curve.\n",
        "\n",
        "Specifically, it is used when:\n",
        "\n",
        "- The data shows a curved or non-linear trend that simple linear regression can't capture.\n",
        "- You want to model complex relationships between variables without moving to entirely different nonlinear models.\n",
        "- You need a smooth curve that fits the data points better than a straight line.\n",
        "- Higher-order interactions between features are important (especially in multivariate cases).\n",
        "- You want to interpolate values within the range of the data (but be cautious when extrapolating beyond the data range).\n",
        "\n",
        "Typical examples:\n",
        "\n",
        "- Growth rates (e.g., population growth over time)  \n",
        "- Price vs. demand relationships  \n",
        "- Modeling trajectories (physics, engineering)  \n",
        "- Predicting trends that rise and fall (like sales over a year)\n",
        "\n",
        "\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "The general equation for polynomial regression is:\n",
        "\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\dots + \\beta_d x^d + \\epsilon\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\( y \\) = dependent variable (target)\n",
        "- \\( x \\) = independent variable (input)\n",
        "- \\( \\beta_0, \\beta_1, \\dots, \\beta_d \\) = coefficients to be learned\n",
        "- \\( d \\) = degree of the polynomial\n",
        "- \\( \\epsilon \\) = error term\n",
        "\n",
        "If you have multiple variables (\\( x_1, x_2, \\dots, x_n \\)), the general form includes powers and interaction terms like:\n",
        "\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_1 x_2 + \\beta_5 x_2^2 + \\dots + \\epsilon\n",
        "\\]\n",
        "\n",
        "\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "Yes, polynomial regression can absolutely be applied to multiple variables** — that's called multivariate polynomial regression.\n",
        "\n",
        "In single-variable polynomial regression, you model the relationship between one input variable \\( x \\) and the output \\( y \\) by fitting a curve like:\n",
        "\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\dots + \\beta_d x^d + \\epsilon\n",
        "\\]\n",
        "\n",
        "In multivariate polynomial regression, you can have several input variables (say \\( x_1, x_2, x_3, \\dots \\)) and include not just powers of each variable but also their interactions.  \n",
        "The model can look like:\n",
        "\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_1 x_2 + \\beta_5 x_2^2 + \\dots + \\epsilon\n",
        "\\]\n",
        "\n",
        "Key points when applying polynomial regression to multiple variables:\n",
        "- Interaction terms (like \\( x_1 x_2 \\)) become important.\n",
        "- Feature explosion: The number of features can grow very quickly as you add degrees and variables. For example, with 2 variables and degree 3, you get terms like \\( x_1^3, x_1^2 x_2, x_1 x_2^2, x_2^3 \\), etc.\n",
        "\n",
        "- Scaling becomes critical because large values raised to powers can dominate the model.\n",
        "\n",
        "- Overfitting risk increases, especially with high-degree polynomials and many variables, so you often need regularization (like Ridge or Lasso regression).\n",
        "\n",
        "In practice, libraries like scikit-learn make it easy. You can use `PolynomialFeatures` to automatically generate these multivariate polynomial terms.\n",
        "\n",
        "\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "•\tOverfitting for high degrees.\n",
        "•\tSensitive to outliers.\n",
        "•\tInterpretation becomes harder with higher-degree polynomials.\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "•\tCross-validation (e.g., k-fold CV).\n",
        "•\tAdjusted R².\n",
        "•\tAIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion).\n",
        "•\tVisual inspection of residual plots.\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "Visualization is important in polynomial regression for several key reasons:\n",
        "\n",
        "1. Understanding the Model Fit:  \n",
        "   Polynomial regression can model complex, non-linear relationships. By visualizing the fitted curve along with the data points, you can quickly assess whether the model captures the underlying trend or if it's underfitting (too simple) or overfitting (too complex).\n",
        "\n",
        "2. Detecting Overfitting and Underfitting:  \n",
        "   Higher-degree polynomials can cause the model to \"wiggle\" excessively to fit the training data, a classic sign of overfitting. Visualization helps you spot such behavior — an overly wavy curve usually indicates overfitting, while a too-straight line might suggest underfitting.\n",
        "\n",
        "3. Communicating Results:  \n",
        "   A visual plot makes it easier for others (and yourself) to understand how well the model explains the relationship between variables. It's much clearer to show a curve fitting the data than to just talk about coefficients and R² scores.\n",
        "\n",
        "4. Choosing the Degree of the Polynomial:  \n",
        "   By plotting different polynomial degrees, you can visually compare and pick the one that best balances bias and variance. It helps guide decisions alongside numerical metrics like cross-validation scores.\n",
        "\n",
        "5. Spotting Outliers and Anomalies:  \n",
        "   Outliers can disproportionately affect polynomial regression. Visualization helps you quickly identify points that might be skewing the model, suggesting you may need to preprocess the data differently.\n",
        "\n",
        "6. Checking for Extrapolation Issues:  \n",
        "   Polynomials can behave wildly outside the range of the data (extrapolation). Visualization shows how the model behaves at the edges, warning you if predictions beyond your data might be unreliable.\n",
        "\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "Typical steps:\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "Example: degree 2 polynomial regression\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X, y)\n",
        "predictions = model.predict(X)\n",
        "\n"
      ],
      "metadata": {
        "id": "L-AOFJG_SCts"
      }
    }
  ]
}